‚ùå ERROR: Analysis failed with exception: litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': "This model's maximum context length is 65536 tokens. However, you requested 119739 tokens (115643 in the messages, 4096 in the completion). Please reduce the length of the messages or completion.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}
Detailed error information:
Traceback (most recent call last):
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py", line 711, in completion
    raise e
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py", line 638, in completion
    self.make_sync_openai_chat_completion_request(
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/logging_utils.py", line 145, in sync_wrapper
    result = func(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py", line 457, in make_sync_openai_chat_completion_request
    raise e
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py", line 439, in make_sync_openai_chat_completion_request
    raw_response = openai_client.chat.completions.with_raw_response.create(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/openai/_legacy_response.py", line 364, in wrapped
    return cast(LegacyAPIResponse[R], func(*args, **kwargs))
                                      ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/openai/_utils/_utils.py", line 279, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py", line 879, in create
    return self._post(
           ^^^^^^^^^^^
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1242, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 919, in request
    return self._request(
           ^^^^^^^^^^^^^^
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/openai/_base_client.py", line 1023, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "This model's maximum context length is 65536 tokens. However, you requested 119739 tokens (115643 in the messages, 4096 in the completion). Please reduce the length of the messages or completion.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/litellm/main.py", line 1692, in completion
    raise e
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/litellm/main.py", line 1665, in completion
    response = openai_chat_completions.completion(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/litellm/llms/openai/openai.py", line 721, in completion
    raise OpenAIError(
litellm.llms.openai.common_utils.OpenAIError: Error code: 400 - {'error': {'message': "This model's maximum context length is 65536 tokens. However, you requested 119739 tokens (115643 in the messages, 4096 in the completion). Please reduce the length of the messages or completion.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/src/pcec/main.py", line 16, in main
    result = pcec.kickoff()
             ^^^^^^^^^^^^^^
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/crewai/crew.py", line 619, in kickoff
    result = self._run_sequential_process()
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/crewai/crew.py", line 731, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/crewai/crew.py", line 829, in _execute_tasks
    task_output = task.execute_sync(
                  ^^^^^^^^^^^^^^^^^^
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/crewai/task.py", line 304, in execute_sync
    return self._execute_core(agent, context, tools)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/crewai/task.py", line 448, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/crewai/task.py", line 368, in _execute_core
    result = agent.execute_task(
             ^^^^^^^^^^^^^^^^^^^
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/crewai/agent.py", line 265, in execute_task
    raise e
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/crewai/agent.py", line 246, in execute_task
    result = self.agent_executor.invoke(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py", line 119, in invoke
    raise e
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py", line 108, in invoke
    formatted_answer = self._invoke_loop()
                       ^^^^^^^^^^^^^^^^^^^
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py", line 166, in _invoke_loop
    raise e
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py", line 146, in _invoke_loop
    answer = self._get_llm_response()
             ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py", line 216, in _get_llm_response
    raise e
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/crewai/agents/crew_agent_executor.py", line 207, in _get_llm_response
    answer = self.llm.call(
             ^^^^^^^^^^^^^^
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/crewai/llm.py", line 310, in call
    response = litellm.completion(**params)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/litellm/utils.py", line 1154, in wrapper
    raise e
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/litellm/utils.py", line 1032, in wrapper
    result = original_function(*args, **kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/litellm/main.py", line 3068, in completion
    raise exception_type(
          ^^^^^^^^^^^^^^^
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 2201, in exception_type
    raise e
  File "/home/ayman/Documents/GitHub/am_ai_policy_ce/pcec/.venv/lib/python3.12/site-packages/litellm/litellm_core_utils/exception_mapping_utils.py", line 282, in exception_type
    raise ContextWindowExceededError(
litellm.exceptions.ContextWindowExceededError: litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: DeepseekException - Error code: 400 - {'error': {'message': "This model's maximum context length is 65536 tokens. However, you requested 119739 tokens (115643 in the messages, 4096 in the completion). Please reduce the length of the messages or completion.", 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_request_error'}}
